// FOR A INSIGHT GENERATION TIME RANGE
// RETURN INSIGHTS CREATED
// EXTRACT LATEST SIGNAL EVENT
// RETURN PARENT SUBQUERY OF SIGNAL RECORDS IN TIME RANGE
// ANNOTATE WITH SIGNAL - INSIGHT TIMINGS
// PROVIDE DIRECT CLICK THROUGH LINK TO RAW EVENT WITH SPECIFIC TIME RANGE

// SIGNAL LEVEL 
_index=sec_signal 

[subquery:

       // INSIGHT LEVEL
    _index=sumologic_system_events _sourcecategory=cseinsight insightcreated
        
    | json field=_raw "insight.riskScore" as riskscore
    | json field=_raw "insight.severityName" as severityname
    | json field=_raw "insightIdentity.readableId" as insightid
    | json field=_raw "insight.name" as name
    | json field=_raw "insight.entityType" as entitytype
    | json field=_raw "insight.entityValue" as entityvalue
    | json field=_raw "insight.tags" as tags
    | json field=_raw "insight.severity" as severity
    | json field=_raw "insight.confidence" as confidence nodrop
    | json field=_raw "insight.signals" as signals
    | json field=_raw "insight.created" as created
 
    | json field=_raw "insight.timeToDetection" as timeToDetection nodrop

    | if (isnull(timeToDetection),-1,round(timeToDetection/3600)) as timeToDetection
    
    // ensure we only store the most recent result
    | max(_messagetime) as _messagetime,max(timeToDetection) as timeToDetection, first(status) as status, first(signals) as signals, first(tags) as tags, max(riskscore) as riskscore,first(severity) as severity, first(severityname) as severityname, first(confidence) as confidence by created,insightid,name,entitytype,entityvalue
    
    // explode embedded signals array
    | replace(signals,",{\"id\":","\n{\"id\":") as signals
    
    // now each signal has a \r \n termination which is good because otherwise super hard to parse them with certainty
    | parse regex field=signals "(?<signal>\{\"id\":\"[^\r\n]+\})" multi 

    | json field=signal "id" as signal_id nodrop
    | json field=signal "created" as signal_created nodrop
    | json field=signal "name" as signal_name nodrop
    | json field=signal "summary" as signal_summary nodrop
    | json field=signal "severity" as signal_severity nodrop
    | json field=signal "ruleId" as ruleid nodrop
    | json field=signal "ruleName" as rulename nodrop
    | concat(ruleid," ", rulename, " sev:",signal_severity) as r
    
   // we should only need the last signal as this is the time the insight was generated and best indicator of any possible lag between ingest and insight creation.
   | sort insightid asc, signal_created asc
   
    // this would save a lookup
    // later we can pull values back out per insight in parent
    | created as insight_created
    | replace(insight_created,"T"," ") as insight_created
    | replace(signal_created,"T"," ") as signal_created
    | concat(formatdate(tolong(_messagetime),"yyyy-MM-dd HH:mm:ss.SSS","ETC/Utc"),"Z") as audit_time
    | name as insight_name
    | last(signal_created) as signal_created,last(signal_id) as signal_id, last(ruleid) as ruleid,last(rulename) as rulename,last(signal_name) as signal_name by insightid,insight_name,entityvalue,insight_created,audit_time,timetodetection,tags,confidence
    | sort insightid asc
    
    // if we save a lookup by id we can pull this info back in later in parent queries
    | save append /temp/lookups/signals_that_created_insight
    
    // compose to parent
    | signal_id as id
    | count by id | compose id

]

// *** CONTINUE SIGNAL LEVEL  ***
| parse regex field=fullrecords "(?<blob>metadata_device.+?\"uid\":\"[^\"]+\")" multi
| parse field = blob "metadata_mapperName\":\"*\"," as mapper nodrop
| parse field = blob "metadata_parseTime\":*," as parsetime nodrop
| parse field = blob "metadata_sourceCategory\":\"*\"," as sourcecategory nodrop
| parse field = blob "metadata_sourceMessageId\":\"*\"," as mid nodrop
| parse field = blob "uid\":\"*\"," as record_uid nodrop
| parse field = blob "timestamp\":*," as event_timestamp nodrop
| _receipttime as sig_received
| count by id,record_uid,mapper,mid,sourcecategory,parsetime,event_timestamp,sig_received,timestamp
| tolong(parsetime) as parsetime
| tolong(event_timestamp ) as event_timestamp
| tolong(timestamp ) as timestamp
| ceil((parsetime - event_timestamp) / (1000)) as t_rec_to_parse
| ceil((sig_received -parsetime ) / (1000)) as t_parse_to_sigr
| ceil((parsetime - timestamp) / (1000 )) as t_parse_to_sigt
| ceil((sig_received - timestamp) / (1000 )) as t_sig_lag_rt

/// enrich with temp insight info lookup in subquery
| lookup insight_created,insight_created,insightid from /temp/lookups/signals_that_created_insight on id=signal_id

| parsedate (insight_created,"yyyy-MM-dd HH:mm:ss.SSS'Z'","ETC/Utc") as ict
| ceil((ict - sig_received) / (1000 )) as t_sigr_to_insight
| ceil((ict - event_timestamp) / (1000 )) as t_rec_to_insight

| formatdate(parsetime,"yyyy-MM-dd HH:mm:ss.SSS'Z'","ETC/Utc") as parsetime
| formatdate(timestamp,"yyyy-MM-dd HH:mm:ss.SSS'Z'","ETC/Utc") as  sig_timestamp
| formatdate(event_timestamp,"yyyy-MM-dd HH:mm:ss.SSS'Z'","ETC/Utc") as  rec_timestamp
| formatdate(sig_received,"yyyy-MM-dd HH:mm:ss.SSS'Z'","ETC/Utc") as  sig_received

| fields -_count,ict,timestamp,event_timestamp
| parsetime as rec_parsetime
| count by id,record_uid,insightid,rec_timestamp,rec_parsetime,t_rec_to_parse,sig_timestamp,sig_received,t_parse_to_sigr,t_parse_to_sigt,t_sig_lag_rt,insight_created,t_sigr_to_insight,t_rec_to_insight,mapper,mid,sourcecategory

// generate raw source link in format: https://xxx.us2.sumologic.com/ui/#/search/@1715298782000,1715298812000@<urlencoded_query>

////////////////////////////////////
// *** your instance of sumo here ***
| "https://service.sumologic.com/ui/#/search/@" as instance
////////////////////////////////////

| parsedate(sig_timestamp,"yyyy-MM-dd HH:mm:ss.SSS'Z'","ETC/Utc") as s

// add a buffer for raw log search time range +/- 60s
| s - 60000 as s
| s + 120000 as e
| format( "%.0f",s) as start
| format( "%.0f",e) as end
| concat("_sourcecategory=",sourcecategory," _messageid=",mid, "\n| formatdate(_receipttime,\"yyyy-MM-dd HH:mm:ss.SSS'Z'\",\"ETC/Utc\") as log_received \n| formatdate(_messagetime,\"yyyy-MM-dd HH:mm:ss.SSS'Z'\",\"ETC/Utc\") as log_timestamp \n| (_receipttime - _messagetime)  / (1000) as ingest_lag_seconds") as raw_q  
| tourl(concat(instance,start,",",end,"@",urlencode(raw_q)),"log search") as raw_log_search
| concat("_sourcecategory=",sourcecategory," \n_messagetid",mid) as raw_meta
| fields -_count,start,end,s,e,raw_q,mid,sourcecategory,instance

| sort insightid asc, rec_timestamp asc
